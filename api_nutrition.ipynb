{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-image food volume estimation\n",
    "Using a  monocular depth estimation network and a segmentation network, we will estimate the volume of the food displayed in the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, model_from_json\n",
    "from keras import backend as K\n",
    "import nest_asyncio\n",
    "from fastapi import FastAPI, UploadFile, File, Depends\n",
    "import uvicorn\n",
    "from pyngrok import ngrok\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Th√™m ƒë∆∞·ªùng d·∫´n c√°c th∆∞ vi·ªán t√πy ch·ªânh\n",
    "sys.path.append('.')\n",
    "sys.path.append('./SAM')\n",
    "sys.path.append('./mmseg')\n",
    "\n",
    "# Import c√°c module c·∫ßn thi·∫øt\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "from FoodSAM.FoodSAM_tools.predict_semantic_mask import semantic_predict\n",
    "from FoodSAM.FoodSAM_tools.enhance_semantic_masks import enhance_masks\n",
    "from food_volume_estimation.volume_estimator import VolumeEstimator\n",
    "from food_volume_estimation.depth_estimation.custom_modules import *\n",
    "\n",
    "# √Åp d·ª•ng nest_asyncio ƒë·ªÉ ch·∫°y FastAPI\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Ki·ªÉm tra GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Bi·∫øn to√†n c·ª•c cho estimator, graph, session\n",
    "estimator = None\n",
    "global_graph = None\n",
    "global_session = None\n",
    "\n",
    "def init_estimator():\n",
    "    \"\"\"Kh·ªüi t·∫°o estimator v·ªõi graph v√† session m·ªõi\"\"\"\n",
    "    global estimator, global_graph, global_session\n",
    "    # X√≥a graph v√† session hi·ªán t·∫°i\n",
    "    K.clear_session()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # T·∫°o graph v√† session m·ªõi\n",
    "    global_graph = tf.Graph()\n",
    "    global_session = tf.Session(graph=global_graph)\n",
    "\n",
    "    with global_graph.as_default():\n",
    "        with global_session.as_default():\n",
    "            K.set_session(global_session)\n",
    "            depth_model_architecture = './models/fine_tune_food_videos/monovideo_fine_tune_food_videos.json'\n",
    "            depth_model_weights = './models/fine_tune_food_videos/monovideo_fine_tune_food_videos.h5'\n",
    "            estimator = VolumeEstimator(arg_init=False)\n",
    "            try:\n",
    "                with open(depth_model_architecture, 'r') as read_file:\n",
    "                    custom_losses = Losses()\n",
    "                    objs = {\n",
    "                        'ProjectionLayer': ProjectionLayer,\n",
    "                        'ReflectionPadding2D': ReflectionPadding2D,\n",
    "                        'InverseDepthNormalization': InverseDepthNormalization,\n",
    "                        'AugmentationLayer': AugmentationLayer,\n",
    "                        'compute_source_loss': custom_losses.compute_source_loss\n",
    "                    }\n",
    "                    model_architecture_json = json.load(read_file)\n",
    "                    estimator.monovideo = model_from_json(model_architecture_json, custom_objects=objs)\n",
    "                estimator._VolumeEstimator__set_weights_trainable(estimator.monovideo, False)\n",
    "                global_session.run(tf.global_variables_initializer())\n",
    "                estimator.monovideo.load_weights(depth_model_weights)\n",
    "                estimator.model_input_shape = estimator.monovideo.inputs[0].shape.as_list()[1:]\n",
    "                depth_net = estimator.monovideo.get_layer('depth_net')\n",
    "                estimator.depth_model = Model(inputs=depth_net.inputs, outputs=depth_net.outputs, name='depth_model')\n",
    "                MIN_DEPTH = 0.01\n",
    "                MAX_DEPTH = 10\n",
    "                estimator.min_disp = 1 / MAX_DEPTH\n",
    "                estimator.max_disp = 1 / MIN_DEPTH\n",
    "                estimator.gt_depth_scale = 0.35\n",
    "                estimator.relax_param = 0.01\n",
    "                print('[*] Loaded depth estimation model.')\n",
    "                # B·ªè graph.finalize() ƒë·ªÉ tr√°nh l·ªói\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "# H√†m ghi log\n",
    "def write_log(message):\n",
    "    log_file_path = \"debug/log.txt\"\n",
    "    with open(log_file_path, \"a\") as log_file:\n",
    "        log_file.write(message + \"\\n\")\n",
    "    print(message)\n",
    "\n",
    "# H√†m x√≥a th∆∞ m·ª•c\n",
    "def clear_folder(folder_path: str):\n",
    "    if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "        for item in os.listdir(folder_path):\n",
    "            item_path = os.path.join(folder_path, item)\n",
    "            try:\n",
    "                if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "                    os.remove(item_path)\n",
    "                elif os.path.isdir(item_path):\n",
    "                    shutil.rmtree(item_path)\n",
    "            except Exception as e:\n",
    "                print(f\"L·ªói khi x√≥a {item_path}: {e}\")\n",
    "        print(f\"‚úÖ ƒê√£ x√≥a to√†n b·ªô n·ªôi dung trong th∆∞ m·ª•c '{folder_path}'.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Th∆∞ m·ª•c '{folder_path}' kh√¥ng t·ªìn t·∫°i.\")\n",
    "\n",
    "# H√†m l∆∞u mask t·ª´ SAM\n",
    "def write_masks_to_folder(masks: List[Dict[str, Any]], path: str) -> None:\n",
    "    header = \"id,area,bbox_x0,bbox_y0,bbox_w,bbox_h,point_input_x,point_input_y,predicted_iou,stability_score,crop_box_x0,crop_box_y0,crop_box_w,crop_box_h\"\n",
    "    metadata = [header]\n",
    "    os.makedirs(os.path.join(path, \"sam_mask\"), exist_ok=True)\n",
    "    masks_array = []\n",
    "    for i, mask_data in enumerate(masks):\n",
    "        mask = mask_data[\"segmentation\"]\n",
    "        masks_array.append(mask.copy())\n",
    "        filename = f\"{i}.png\"\n",
    "        cv2.imwrite(os.path.join(path, \"sam_mask\", filename), mask * 255)\n",
    "        mask_metadata = [\n",
    "            str(i),\n",
    "            str(mask_data[\"area\"]),\n",
    "            *[str(x) for x in mask_data[\"bbox\"]],\n",
    "            *[str(x) for x in mask_data[\"point_coords\"][0]],\n",
    "            str(mask_data[\"predicted_iou\"]),\n",
    "            str(mask_data[\"stability_score\"]),\n",
    "            *[str(x) for x in mask_data[\"crop_box\"]],\n",
    "        ]\n",
    "        row = \",\".join(mask_metadata)\n",
    "        metadata.append(row)\n",
    "    masks_array = np.stack(masks_array, axis=0)\n",
    "    np.save(os.path.join(path, \"sam_mask\", \"masks.npy\"), masks_array)\n",
    "    metadata_path = os.path.join(path, \"sam_metadata.csv\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(metadata))\n",
    "\n",
    "# H√†m ch·∫°y FoodSAM (PyTorch)\n",
    "def run_foodsam(image_path):\n",
    "    args = argparse.Namespace(\n",
    "        img_path=image_path,\n",
    "        output=\"Output/Semantic_Results\",\n",
    "        device=device,\n",
    "        SAM_checkpoint=\"ckpts/sam_vit_h_4b8939.pth\",\n",
    "        semantic_config=\"configs/SETR_MLA_768x768_80k_base.py\",\n",
    "        semantic_checkpoint=\"ckpts/SETR_MLA/iter_80000.pth\",\n",
    "        model_type=\"vit_h\",\n",
    "        color_list_path=\"FoodSAM/FoodSAM_tools/color_list.npy\",\n",
    "        category_txt=\"FoodSAM/FoodSAM_tools/category_id_files/foodseg103_category_id.txt\",\n",
    "        num_class=104\n",
    "    )\n",
    "\n",
    "    os.makedirs(args.output, exist_ok=True)\n",
    "    write_log(\"Running SAM...\")\n",
    "\n",
    "    # Load m√¥ h√¨nh SAM\n",
    "    sam = sam_model_registry[args.model_type](checkpoint=args.SAM_checkpoint)\n",
    "    sam.to(device=args.device)\n",
    "    write_log(f\"SAM model loaded on {args.device}\")\n",
    "\n",
    "    # Thi·∫øt l·∫≠p b·ªô t·∫°o mask\n",
    "    output_mode = \"binary_mask\"\n",
    "    generator = SamAutomaticMaskGenerator(sam, output_mode=output_mode)\n",
    "\n",
    "    # ƒê·ªçc ·∫£nh v√† th·ª±c hi·ªán segmentation\n",
    "    targets = [args.img_path]\n",
    "    for t in targets:\n",
    "        write_log(f\"Processing {t}...\")\n",
    "        image = cv2.imread(t)\n",
    "        if image is None:\n",
    "            write_log(f\"Kh√¥ng th·ªÉ t·∫£i {t}, b·ªè qua...\")\n",
    "            continue\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        masks = generator.generate(image)\n",
    "\n",
    "        # L∆∞u k·∫øt qu·∫£\n",
    "        base = os.path.basename(t).split('.')[0]\n",
    "        save_base = os.path.join(args.output, base)\n",
    "        os.makedirs(save_base, exist_ok=True)\n",
    "        write_masks_to_folder(masks, save_base)\n",
    "        shutil.copyfile(t, os.path.join(save_base, \"input.jpg\"))\n",
    "\n",
    "    write_log(\"SAM processing done!\")\n",
    "    write_log(\"Running semantic segmentation model...\")\n",
    "    semantic_predict(args.semantic_config, args.semantic_checkpoint,\n",
    "                     args.output, args.color_list_path, args.img_path, device=args.device)\n",
    "    write_log(\"Semantic segmentation done!\")\n",
    "    write_log(\"Enhancing semantic masks...\")\n",
    "    enhance_masks(args.output, args.category_txt, args.color_list_path, num_class=args.num_class)\n",
    "    write_log(\"Enhancement done!\")\n",
    "    write_log(f\"Results saved in {args.output}!\")\n",
    "    return base\n",
    "\n",
    "# H√†m ch·∫°y depth estimation v√† t√≠nh kh·ªëi l∆∞·ª£ng (TensorFlow)\n",
    "def run_depth_estimation(image_path, base):\n",
    "    global estimator, global_graph, global_session\n",
    "    # Kh·ªüi t·∫°o l·∫°i estimator v·ªõi graph/session m·ªõi\n",
    "    init_estimator()\n",
    "    with global_graph.as_default():\n",
    "        with global_session.as_default():\n",
    "            K.set_session(global_session)\n",
    "            # B·ªè log summary c·ªßa depth model theo y√™u c·∫ßu\n",
    "\n",
    "            # Ch·∫°y ∆∞·ªõc l∆∞·ª£ng th·ªÉ t√≠ch\n",
    "            plate_diameter = 0  # B·ªè qua ph√°t hi·ªán ƒëƒ©a\n",
    "            outputs_list, food_volumes = estimator.estimate_volume(\n",
    "                image_path, fov=70, plate_diameter_prior=plate_diameter,\n",
    "                plot_results=True, para_folder_path=f\"Output/Semantic_Results/{base}/masks/\"\n",
    "            )\n",
    "\n",
    "            # Chuy·ªÉn ƒë·ªïi th·ªÉ t√≠ch sang kh·ªëi l∆∞·ª£ng\n",
    "            food_masses = estimator.convert_volume_to_mass('Density_sub_90.xlsx', food_volumes)\n",
    "            write_log(f\"Food masses: {food_masses}\")\n",
    "\n",
    "            # Nh·∫≠p API_KEY c·ªßa b·∫°n\n",
    "            API_KEY = \"gP07Q5U7ULsiXCXLbCGVqk4c2Y6Yx39nMWJiuJxx\"\n",
    "            \n",
    "            # L·∫•y d·ªØ li·ªáu dinh d∆∞·ª°ng\n",
    "            nutrition_data = estimator.get_nutrition_for_all(food_masses, API_KEY)\n",
    "            write_log(f\"nutrition_data: {nutrition_data}\")\n",
    "            return nutrition_data\n",
    "\n",
    "# H√†m x·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o\n",
    "def process_image_input(image: UploadFile, image_path: str):\n",
    "    if image:\n",
    "        input_folder = \"Input\"\n",
    "        os.makedirs(input_folder, exist_ok=True)\n",
    "        clear_folder(\"Input\")\n",
    "        clear_folder(\"debug\")\n",
    "        temp_file_path = os.path.join(input_folder, image.filename)\n",
    "        with open(temp_file_path, \"wb\") as temp_file:\n",
    "            shutil.copyfileobj(image.file, temp_file)\n",
    "        return temp_file_path, True\n",
    "    elif image_path:\n",
    "        clear_folder(\"Input\")\n",
    "        clear_folder(\"debug\")\n",
    "        if not os.path.exists(image_path):\n",
    "            raise ValueError(f\"The image at path '{image_path}' does not exist.\")\n",
    "        write_log(f\"Using image path: {image_path}\")\n",
    "        return image_path, False\n",
    "    else:\n",
    "        raise ValueError(\"Please provide either an uploaded image or an image path.\")\n",
    "\n",
    "# Kh·ªüi t·∫°o FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# ƒê·∫£m b·∫£o th∆∞ m·ª•c debug t·ªìn t·∫°i\n",
    "os.makedirs(\"debug\", exist_ok=True)\n",
    "\n",
    "# Endpoint FastAPI (chuy·ªÉn sang ƒë·ªìng b·ªô)\n",
    "@app.post(\"/process-image/\", dependencies=[Depends(lambda: None)])\n",
    "def process_image(image: UploadFile = File(None), image_path: str = None):\n",
    "    try:\n",
    "        # X·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o\n",
    "        image_path_to_process, is_uploaded = process_image_input(image, image_path)\n",
    "\n",
    "        # B∆∞·ªõc 1: Ch·∫°y FoodSAM (PyTorch)\n",
    "        base = run_foodsam(image_path_to_process)\n",
    "\n",
    "        # B∆∞·ªõc 2: Ch·∫°y depth estimation (TensorFlow)\n",
    "        food_masses = run_depth_estimation(image_path_to_process, base)\n",
    "\n",
    "        # X√≥a file t·∫°m n·∫øu c√≥\n",
    "        if is_uploaded:\n",
    "            os.remove(image_path_to_process)\n",
    "\n",
    "        return {\"food_nutrition\": food_masses}\n",
    "    except Exception as e:\n",
    "        write_log(f\"Error: {str(e)}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# H√†m ch·∫°y FastAPI server\n",
    "def start_uvicorn():\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n",
    "\n",
    "# Ch·∫°y FastAPI trong thread ri√™ng\n",
    "threading.Thread(target=start_uvicorn, daemon=True).start()\n",
    "\n",
    "# Thi·∫øt l·∫≠p Ngrok\n",
    "ngrok.set_auth_token(\"2w4ttBJNFtLVvtpgIKDbM3ord6S_4ZhiZ62czznikvh3rUnRM\")\n",
    "public_url = ngrok.connect(8000)\n",
    "\n",
    "# Log v√† hi·ªÉn th·ªã URL c√¥ng c·ªông\n",
    "write_log(f\"Public URL: {public_url}\")\n",
    "print(\"üöÄ Public FastAPI server is running at:\", public_url)\n",
    "\n",
    "# Gi·ªØ thread ch√≠nh ch·∫°y\n",
    "while True:\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r\"rice_1.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List\n",
    "import argparse\n",
    "\n",
    "# Th√™m ƒë∆∞·ªùng d·∫´n c√°c th∆∞ vi·ªán t√πy ch·ªânh\n",
    "sys.path.append('.')\n",
    "sys.path.append('./SAM')\n",
    "sys.path.append('./mmseg')\n",
    "\n",
    "# Import c√°c module c·∫ßn thi·∫øt\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "from FoodSAM.FoodSAM_tools.predict_semantic_mask import semantic_predict\n",
    "from FoodSAM.FoodSAM_tools.enhance_semantic_masks import enhance_masks\n",
    "from FoodSAM.FoodSAM_tools.evaluate_foodseg103 import evaluate\n",
    "\n",
    "\n",
    "# T·∫°o ƒë·ªëi t∆∞·ª£ng args thay th·∫ø argparse\n",
    "args = argparse.Namespace(\n",
    "    img_path = image_path,  # C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n ·∫£nh\n",
    "    output=\"Output/Semantic_Results\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    SAM_checkpoint=\"ckpts/sam_vit_h_4b8939.pth\",\n",
    "    semantic_config=\"configs/SETR_MLA_768x768_80k_base.py\",\n",
    "    semantic_checkpoint=\"ckpts/SETR_MLA/iter_80000.pth\",\n",
    "    model_type=\"vit_h\",\n",
    "    color_list_path=\"FoodSAM/FoodSAM_tools/color_list.npy\",\n",
    "    category_txt=\"FoodSAM/FoodSAM_tools/category_id_files/foodseg103_category_id.txt\",\n",
    "    num_class=104\n",
    ")\n",
    "\n",
    "\n",
    "def write_masks_to_folder(masks: List[Dict[str, Any]], path: str) -> None:\n",
    "    \"\"\"L∆∞u c√°c mask c·ªßa SAM v√†o th∆∞ m·ª•c\"\"\"\n",
    "    header = \"id,area,bbox_x0,bbox_y0,bbox_w,bbox_h,point_input_x,point_input_y,predicted_iou,stability_score,crop_box_x0,crop_box_y0,crop_box_w,crop_box_h\"  # noqa\n",
    "    metadata = [header]\n",
    "    os.makedirs(os.path.join(path, \"sam_mask\"), exist_ok=True)\n",
    "    masks_array = []\n",
    "    for i, mask_data in enumerate(masks):\n",
    "        mask = mask_data[\"segmentation\"]\n",
    "        masks_array.append(mask.copy())\n",
    "        filename = f\"{i}.png\"\n",
    "        cv2.imwrite(os.path.join(path, \"sam_mask\", filename), mask * 255)\n",
    "        mask_metadata = [\n",
    "            str(i),\n",
    "            str(mask_data[\"area\"]),\n",
    "            *[str(x) for x in mask_data[\"bbox\"]],\n",
    "            *[str(x) for x in mask_data[\"point_coords\"][0]],\n",
    "            str(mask_data[\"predicted_iou\"]),\n",
    "            str(mask_data[\"stability_score\"]),\n",
    "            *[str(x) for x in mask_data[\"crop_box\"]],\n",
    "        ]\n",
    "        row = \",\".join(mask_metadata)\n",
    "        metadata.append(row)\n",
    "\n",
    "    masks_array = np.stack(masks_array, axis=0)\n",
    "    np.save(os.path.join(path, \"sam_mask\", \"masks.npy\"), masks_array)\n",
    "    metadata_path = os.path.join(path, \"sam_metadata.csv\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(metadata))\n",
    "\n",
    "\n",
    "\n",
    "def create_logger(save_folder):\n",
    "    \"\"\"T·∫°o logger ƒë·ªÉ ghi log trong qu√° tr√¨nh ch·∫°y\"\"\"\n",
    "    log_file = \"sam_process.log\"\n",
    "    final_log_file = os.path.join(save_folder, log_file)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format='[%(asctime)s] [%(filename)s:%(lineno)d] [%(levelname)s] %(message)s',\n",
    "        level=logging.INFO,\n",
    "        handlers=[\n",
    "            logging.FileHandler(final_log_file, mode='w'),\n",
    "            logging.StreamHandler()\n",
    "        ])\n",
    "    logger = logging.getLogger()\n",
    "    print(f\"Logger created: {final_log_file}\")\n",
    "    return logger\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"Ch·∫°y qu√° tr√¨nh segmentation\"\"\"\n",
    "    os.makedirs(args.output, exist_ok=True)\n",
    "    # logger = create_logger(args.output)\n",
    "    # logger.info(\"Running SAM...\")\n",
    "\n",
    "    # Ki·ªÉm tra thi·∫øt b·ªã c√≥ h·ªó tr·ª£ CUDA kh√¥ng\n",
    "    if args.device == \"cuda\" and not torch.cuda.is_available():\n",
    "        # logger.warning(\"CUDA is not available. Switching to CPU.\")\n",
    "        args.device = \"cpu\"\n",
    "\n",
    "    # Load m√¥ h√¨nh SAM\n",
    "    sam = sam_model_registry[args.model_type](checkpoint=args.SAM_checkpoint)\n",
    "    _ = sam.to(device=args.device)\n",
    "\n",
    "    # Thi·∫øt l·∫≠p b·ªô t·∫°o mask\n",
    "    output_mode = \"binary_mask\"\n",
    "    generator = SamAutomaticMaskGenerator(sam, output_mode=output_mode)\n",
    "\n",
    "    assert args.img_path, \"B·∫°n ph·∫£i cung c·∫•p ƒë∆∞·ªùng d·∫´n ·∫£nh.\"\n",
    "    \n",
    "    # ƒê·ªçc ·∫£nh v√† th·ª±c hi·ªán segmentation\n",
    "    targets = [args.img_path]\n",
    "    for t in targets:\n",
    "        # logger.info(f\"Processing {t}...\")\n",
    "        image = cv2.imread(t)\n",
    "        if image is None:\n",
    "            # logger.error(f\"Kh√¥ng th·ªÉ t·∫£i {t}, b·ªè qua...\")\n",
    "            continue\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        masks = generator.generate(image)\n",
    "\n",
    "        # L∆∞u k·∫øt qu·∫£\n",
    "        base = os.path.basename(t).split('.')[0]\n",
    "        save_base = os.path.join(args.output, base)\n",
    "        os.makedirs(save_base, exist_ok=True)\n",
    "        write_masks_to_folder(masks, save_base)\n",
    "        shutil.copyfile(t, os.path.join(save_base, \"input.jpg\"))\n",
    "\n",
    "    # logger.info(\"SAM processing done!\\n\")\n",
    "\n",
    "    # Ch·∫°y m√¥ h√¨nh ph√¢n ƒëo·∫°n ng·ªØ nghƒ©a\n",
    "    # logger.info(\"Running semantic segmentation model...\")\n",
    "    semantic_predict(args.semantic_config, args.semantic_checkpoint,\n",
    "                     args.output, args.color_list_path, args.img_path, device=args.device)\n",
    "    # logger.info(\"Semantic segmentation done!\\n\")\n",
    "\n",
    "    # TƒÉng c∆∞·ªùng segmentation mask\n",
    "    # logger.info(\"Enhancing semantic masks...\")\n",
    "    enhance_masks(args.output, args.category_txt, args.color_list_path, num_class=args.num_class)\n",
    "    # logger.info(\"Enhancement done!\\n\")\n",
    "\n",
    "    # logger.info(f\"Results saved in {args.output}!\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def clear_folder(folder_path: str):\n",
    "    \"\"\"\n",
    "    X√≥a to√†n b·ªô n·ªôi dung b√™n trong th∆∞ m·ª•c (t·∫•t c·∫£ file, th∆∞ m·ª•c con, symbolic link),\n",
    "    nh∆∞ng v·∫´n gi·ªØ l·∫°i th∆∞ m·ª•c g·ªëc.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c c·∫ßn x√≥a n·ªôi dung.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "        # Duy·ªát qua t·∫•t c·∫£ file v√† th∆∞ m·ª•c con trong folder\n",
    "        for item in os.listdir(folder_path):\n",
    "            item_path = os.path.join(folder_path, item)\n",
    "            try:\n",
    "                if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "                    os.remove(item_path)  # X√≥a file ho·∫∑c symbolic link\n",
    "                elif os.path.isdir(item_path):\n",
    "                    shutil.rmtree(item_path)  # X√≥a th∆∞ m·ª•c con v√† n·ªôi dung b√™n trong\n",
    "            except Exception as e:\n",
    "                print(f\"L·ªói khi x√≥a {item_path}: {e}\")\n",
    "        print(f\"‚úÖ ƒê√£ x√≥a to√†n b·ªô n·ªôi dung trong th∆∞ m·ª•c '{folder_path}'.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Th∆∞ m·ª•c '{folder_path}' kh√¥ng t·ªìn t·∫°i.\")\n",
    "\n",
    "\n",
    "# clear_folder(\"masks\")\n",
    "# clear_folder(\"Output\")\n",
    "# # Ch·∫°y ch∆∞∆°ng tr√¨nh\n",
    "# main(args)\n",
    "print(\"doneeeeee\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOOD VOLUME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from keras.models import Model, model_from_json\n",
    "from food_volume_estimation.volume_estimator import VolumeEstimator\n",
    "from food_volume_estimation.depth_estimation.custom_modules import *\n",
    "from food_volume_estimation.food_segmentation.food_segmentator import FoodSegmentator\n",
    "import matplotlib.pyplot as plt\n",
    "from pyntcloud import PyntCloud\n",
    "\n",
    "# Paths to model archiecture/weights\n",
    "depth_model_architecture = './models/fine_tune_food_videos/monovideo_fine_tune_food_videos.json'\n",
    "depth_model_weights = './models/fine_tune_food_videos/monovideo_fine_tune_food_videos.h5'\n",
    "print(\"loaded model estimate volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator object and intialize\n",
    "estimator = VolumeEstimator(arg_init=False)\n",
    "with open(depth_model_architecture, 'r') as read_file:\n",
    "    custom_losses = Losses()\n",
    "    objs = {'ProjectionLayer': ProjectionLayer,\n",
    "            'ReflectionPadding2D': ReflectionPadding2D,\n",
    "            'InverseDepthNormalization': InverseDepthNormalization,\n",
    "            'AugmentationLayer': AugmentationLayer,\n",
    "            'compute_source_loss': custom_losses.compute_source_loss}\n",
    "    model_architecture_json = json.load(read_file)\n",
    "    estimator.monovideo = model_from_json(model_architecture_json, custom_objects=objs)\n",
    "estimator._VolumeEstimator__set_weights_trainable(estimator.monovideo, False)\n",
    "estimator.monovideo.load_weights(depth_model_weights)\n",
    "estimator.model_input_shape = estimator.monovideo.inputs[0].shape.as_list()[1:]\n",
    "depth_net = estimator.monovideo.get_layer('depth_net')\n",
    "estimator.depth_model = Model(inputs=depth_net.inputs, outputs=depth_net.outputs, name='depth_model')\n",
    "print('[*] Loaded depth estimation model.')\n",
    "\n",
    "# Depth model configuration\n",
    "MIN_DEPTH = 0.01\n",
    "MAX_DEPTH = 10\n",
    "estimator.min_disp = 1 / MAX_DEPTH\n",
    "estimator.max_disp = 1 / MIN_DEPTH\n",
    "estimator.gt_depth_scale = 0.35 # Ground truth expected median depth\n",
    "\n",
    "# Create segmentator object\n",
    "# estimator.segmentator = FoodSegmentator(segmentation_model_weights)\n",
    "\n",
    "# Set plate adjustment relaxation parameter\n",
    "estimator.relax_param = 0.01\n",
    "print(\"done set up model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUTRITION INFORMATION RETRIEAVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H√†m ghi log v√†o file\n",
    "def write_log(message):\n",
    "    log_dir = \"debug\"\n",
    "    log_file_path = os.path.join(log_dir, \"log.txt\")\n",
    "\n",
    "    # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Ghi log\n",
    "    with open(log_file_path, \"a\") as log_file:\n",
    "        log_file.write(message + \"\\n\")\n",
    "        \n",
    "def run(image_path_run):\n",
    "    write_log(f\"Starting processing for image: {image_path_run}\")\n",
    "    # T·∫°o ƒë·ªëi t∆∞·ª£ng args thay th·∫ø argparse\n",
    "    args = argparse.Namespace(\n",
    "        img_path=image_path_run,  # C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n ·∫£nh\n",
    "        output=\"Output/Semantic_Results\",\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        SAM_checkpoint=\"ckpts/sam_vit_h_4b8939.pth\",\n",
    "        semantic_config=\"configs/SETR_MLA_768x768_80k_base.py\",\n",
    "        semantic_checkpoint=\"ckpts/SETR_MLA/iter_80000.pth\",\n",
    "        model_type=\"vit_h\",\n",
    "        color_list_path=\"FoodSAM/FoodSAM_tools/color_list.npy\",\n",
    "        category_txt=\"FoodSAM/FoodSAM_tools/category_id_files/foodseg103_category_id.txt\",\n",
    "        num_class=104\n",
    "    )\n",
    "    \n",
    "\n",
    "    clear_folder(\"masks\")\n",
    "    clear_folder(\"Output\")\n",
    "    # Ch·∫°y ch∆∞∆°ng tr√¨nh\n",
    "    main(args)\n",
    "\n",
    "    name_without_ext = os.path.splitext(image_path_run)[0]\n",
    "    write_log(f\"Processed image name without extension: {name_without_ext}\")\n",
    "    # name_without_ext_without_input = name_without_ext.split(\"Input/\")[1]\n",
    "    # write_log(f\"name_without_ext_without_input: {name_without_ext_without_input}\")\n",
    "\n",
    "    plate_diameter = 0  # Set as 0 to ignore plate detection and scaling\n",
    "    outputs_list, food_volumes = estimator.estimate_volume(image_path_run, fov=70, plate_diameter_prior=plate_diameter,\n",
    "                                                        plot_results=True, para_folder_path=rf\"Output/Semantic_Results/{name_without_ext}/masks/\")\n",
    "\n",
    "    food_masses = estimator.convert_volume_to_mass(\n",
    "        r'Density_sub_90.xlsx', food_volumes)\n",
    "    \n",
    "    return food_masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run(r\"rice_1.jpg\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit_env",
   "language": "python",
   "name": "vit_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
